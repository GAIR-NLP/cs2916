"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4949],{6828:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var r=t(4848),i=t(8453);const s={sidebar_position:7},a="Parallel Training of Large Language Models",o={id:"lectures/instruction-tuning",title:"Parallel Training of Large Language Models",description:"\u8bfe\u7a0b\u5927\u7eb2",source:"@site/docs/lectures/7-instruction-tuning.md",sourceDirName:"lectures",slug:"/lectures/instruction-tuning",permalink:"/cs2916/docs/lectures/instruction-tuning",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/lectures/7-instruction-tuning.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"Evaluation",permalink:"/cs2916/docs/lectures/evaluation"},next:{title:"Instruction Tuning and Alignment",permalink:"/cs2916/docs/lectures/rm-rlhf"}},l={},c=[{value:"\u8bfe\u7a0b\u5927\u7eb2",id:"\u8bfe\u7a0b\u5927\u7eb2",level:2},{value:"\u63a8\u8350\u9605\u8bfb\u6750\u6599",id:"\u63a8\u8350\u9605\u8bfb\u6750\u6599",level:2}];function u(e){const n={a:"a",h1:"h1",h2:"h2",li:"li",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"parallel-training-of-large-language-models",children:"Parallel Training of Large Language Models"}),"\n",(0,r.jsx)(n.h2,{id:"\u8bfe\u7a0b\u5927\u7eb2",children:"\u8bfe\u7a0b\u5927\u7eb2"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u5927\u6a21\u578b\u5e76\u884c\u8bad\u7ec3 [",(0,r.jsx)(n.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:t(4991).A+"",children:"\u8bfe\u4ef6"}),"]"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"\u63a8\u8350\u9605\u8bfb\u6750\u6599",children:"\u63a8\u8350\u9605\u8bfb\u6750\u6599"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["[\u8bba\u6587]",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2205.14135.pdf",children:" Fast and Memory-Efficient Exact Attention\nwith IO-Awareness"})]}),"\n",(0,r.jsxs)(n.li,{children:["[\u8bba\u6587]",(0,r.jsx)(n.a,{href:"https://browse.arxiv.org/pdf/2310.01889",children:"Ring Attention with Blockwise\nTransformers for Near-Infinite Context"})]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},4991:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/files/lecture07-LLM-parallel-9a1132dd093c55e12f0932867b72ba00.pdf"},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);